{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMQbsK+NGMUM1UXhRcjEf94"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- 가중치와 절편은 데이터에서 학습됩니다\n","- f는 활성화 함수 , w는 가중치, b는 절편\n","- y = f(wx + b) (x와 w는 벡터, x와 w의 곱셈은 점곱 dot product)\n","- 선형 함수 표현인 wx + b를 아핀 변환(affine transform)이라고도 부릅니다.\n","- f로 표시한 활성화 함수는 일반적으로 비선형 함수\n","- 퍼셉트론은 선형 함수와 비선형 함수의 조합\n"],"metadata":{"id":"RR70Y4XjXmSo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvUXFqqfXPim"},"outputs":[],"source":["# 코드3-1. 파이토치로 구현한 퍼셉트론\n","\n","import torch\n","import torch.nn as nn\n","\n","# torch.nn은 Linear 클래스를 제공\n","# 가중치와 절편은 nn.Linear 클래스 안에서 관리됨\n","# 절편이 필요없는 모델이 필요할 경우, nn.Linear 생성자 호출 시 bias=False로 지정 가능\n","\n","class Perceptron(nn.Module):\n","    \"\"\"퍼셉트론은 하나의 선형 층 입니다\"\"\"\n","    def _init_(self, input_dim):\n","        \"\"\"\n","        매개변수:\n","            input_dim (int): 입력 특성의 크기\n","        \"\"\"\n","        super(Perceptron, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x_in):\n","        \"\"\"\n","        퍼셉트론의 정방향 계산\n","\n","        매개변수:\n","            x_in (torch.Tensor): 입력 데이터 텐서\n","                x_in.shape는 (batch, num_features)입니다.\n","        변환값:\n","            결과 텐서. tensor.shape는 (batch,)입니다.\n","        \"\"\"\n","        return torch.sigmoid(self.fc1(x_in)).squeeze()\n","\n","\n"]},{"cell_type":"markdown","source":["- 활성화 함수는 비선형 함수\n","- 신경망에서 데이터의 복잡한 관계를 감지하는데 사용함"],"metadata":{"id":"3GtP7HNfZ-JB"}},{"cell_type":"markdown","source":["### 3.2.1 시그모이드\n","\n","- 시그모이드는 초기 활성화함수\n","- 임의의 실숫값을 받아 0과 1사이의 범위로 압축함\n","- 시그모이드는 극단적인 출력을 만듬 (그레이디언트가 0 또는 발산 -> 문제: 그레디이언트 소실 문제라고함: vanishing gradient problem)\n","- 신경망에서 시그모이드 활성화 함수는 거의 출력층에서만 사용됨\n","- 출력층에서는 출력을 확률로 압축하는데 시그모이드 함수를 사용함\n","\n"],"metadata":{"id":"ZbKOVhhlap3J"}},{"cell_type":"code","source":["# 코드3-2. 시그모이드 활성화 함수\n","\n","import torch\n","import matplotlib.pyplot as plt\n","\n","x = torch.range(-5., 5., 0.1)\n","y = torch.sigmoid(x)\n","plt.plot(x.numpy(), y.numpy())\n","plt.show()\n"],"metadata":{"id":"yqQVFyA1Z4ly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2.2 하이볼릭 탄젠트"],"metadata":{"id":"5Ajrk2JmcnA9"}},{"cell_type":"code","source":[],"metadata":{"id":"tFFzO0PObpN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드3-9. 이진 크로스 엔트로피 손실"],"metadata":{"id":"kz4woEYw9kY5"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","bce_loss = nn.BCELoss()\n","sigmoid = nn.Sigmoid()\n","probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n","targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4,1)\n","loss = bce_loss(probabilities, targets)\n","print(probabilities)\n","print(loss)\n"],"metadata":{"id":"W1GM7xZg9nNW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드 3-10. Adam 옵티마이저 준비"],"metadata":{"id":"j0-eLXgSFKEM"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","input_dim = 2\n","lr = 0.001\n","\n","perception = Perception(input_dim=input_dim)\n","bce_loss = nn.BCELoss()\n","optimizer = optim.Adam(params=perception.parameters(), lr=lr)\n"],"metadata":{"id":"LtUaqncN-3pB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드 3-11. 퍼셉트론과 이진 분류를 위한 지도 학습 훈련 반복"],"metadata":{"id":"QCmNolbYFxL1"}},{"cell_type":"code","source":["# 각 에포크는 전체 훈련 데이터를 사용합니다\n","for epoch_i in range(n_epochs):\n","    # 내부 반복은 데이터셋에 있는 배치에 대해 수행됩니다\n","    for batch_i in range(n_batches):\n","\n","        # 0단계: 데이터 가져오기\n","        x_data, y_target = get_toy_data(batch_size)\n","\n","        # 1단계: 그레이디언트 초기화\n","        perceptron.zero_grad()\n","\n","        # 2단계: 모델의 정방향 계산 수행하기\n","        y_pred = perceptron(x_data, apply_sigmoid=True)\n","\n","        # 3단계: 최적화하려는 손실 계산하기\n","        loss = bce_loss(y_pred, y_target)\n","\n","        # 4단계: 손실 신호를 거꾸로 전파하기\n","        loss.backward()\n","        # 5단계: 옵티마이저로 업데이트하기\n","        optimizer.step()\n","\n"],"metadata":{"id":"1RpemTX6F1J5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.6.1 옐프 리뷰 데이터셋\n","코드 3-12. 훈련, 검증, 테스트 세트 만들기"],"metadata":{"id":"Th9FVmDDKLJL"}},{"cell_type":"code","source":["# 별점 기준으로 나누어 훈련, 검증, 테스트를 만듭니다.\n","by_rating = collections.defaultdict(list)\n","for _, row in review_subset.iterrows():\n","    by_rating[row.rating].append(row.to_dict())\n","\n","# 분할 데이터를 만듭니다.\n","final_list = []\n","np.random.seed(args.seed)\n","\n","for _, item_list in sorted(by_rating.items()):\n","    np.random.shuffle(item_list)\n","    n_total = len(item_list)\n","    n_train = int(args.train_proportion * n_total)\n","    n_val = int(args.val_proportion * n_total)\n","    n_test = int(args.test_proportion * n_total)\n","\n","    # 데이터 포인터에 분할 속성을 추가합니다\n","    for item in item_list[:n_train]:\n","        item['split'] = 'train'\n","\n","    for item in item_list[n_train:n_train+n_val]:\n","        item['split'] = 'val'\n","\n","    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n","        item['split'] = 'test'\n","\n","    # 최종 리스트에 추가합니다\n","    final_list.extend(item_list)\n","\n","final_reviews = pd.DataFrame(final_list)"],"metadata":{"id":"VM71I4t0J0A_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드 3-13. 최소한의 데이터 정제 작업"],"metadata":{"id":"jGv-N8pGvzXc"}},{"cell_type":"code","source":["import re\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"([.,!?])\", r\" \\1 \", text) # 구두점 기호 앞뒤에 공백을 넣고\n","    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text) # 구두점이 아닌 기호를 제거하는 데이터 정제 작업을 모든 세트에 수행\n","\n","final_reviews.review = final_reviews.review.apply(preprocess_text)\n"],"metadata":{"id":"RQ1BM6bKwUzh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드 3-14. 옐프 리뷰 데이터를 위한 파이토치 데이터셋 클래스"],"metadata":{"id":"d_P5r118yEL9"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, review_df, vectorizer):\n","        \"\"\"\n","        매개변수:\n","            review_df (pandas.DataFrame): 데이터셋\n","            vectorizer (ReviewVectorizer): ReviewVectorizer 객체\n","        \"\"\"\n","        self.review_df = review_df\n","        self._vectorizer = vectorizer\n","\n","        self.train_df = self.review_df[self.review_df.split == 'train']\n","        self.train_size = len(self.train_df)\n","\n","        self.val_df = self.review_df[self.review_df.split == 'val']\n","        self.validation_size = len(self.val_df)\n","\n","        self.test_df = self.review_df[self.review_df.split == 'test']\n","        self.test_size = len(self.test_df)\n","\n","        self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                             'val': (self.val_df, self.validation_size),\n","                             'test': (self.test_df, self.test_size)}\n","\n","        self.set_split('train')\n","\n","    @classmethod\n","    def load_dataset_and_make_vectorizer(cls, review_csv):\n","        \"\"\" 데이터셋을 로드하고 새로운 ReviewVectorizer 객체를 만듭니다\n","\n","        매개변수:\n","            review_csv (str): 데이터셋의 위치\n","        반환값:\n","            ReviewDataset의 인스턴스\n","        \"\"\"\n","        review_df = pd.read(review_csv)\n","        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n","\n","    def get_vectorizer(self):\n","        \"\"\" ReviewVectorizer 객체를 반환합니다 \"\"\"\n","        return self._vectorizer\n","\n","    def set_split(self, split=\"train\"):\n","        \"\"\" 데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다\n","\n","        매개변수:\n","            split (str): \"train\", \"val\", \"test\" 중 하나\n","        \"\"\"\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","\n","    def __len__(self):\n","        return self._target_size\n","\n","    def __getitem__(self, index):\n","        \"\"\" 파이토치 데이터세세의 주요 진입 메서드\n","\n","        매개변수:\n","            index (int): 데이터 포인트의 인덱스\n","        반환값:\n","            데이터 포인트의 특성(x_data)과 레이블(y_target)로 이루어진 딕셔너리\n","        \"\"\"\n","        row = self._target_df.iloc[index]\n","\n","        review_vector = \\\n","            self._vectorizer.vectorize(row.review)\n","\n","        ratin_index = \\\n","            self._vectorizer.rating_vocab.lookup_token(row.rating)\n","\n","        return {'x_data': review_vector,\n","                'y_target': rating_index}\n","\n","    def get_num_batches(self, batch_size):\n","        \"\"\" 배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n","\n","        매개변수:\n","            batch_size (int)\n","        반환값:\n","            배치 개수\n","        \"\"\"\n","        return len(self) // batch_size"],"metadata":{"id":"y1kpW4mYxBNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["코드3-15. 머신러닝 파이프라인에 필요한 토큰과 정수 매핑을 관리하는 Vocabulary 클래스"],"metadata":{"id":"zjmGlnLrpDz9"}},{"cell_type":"code","source":["class Vocabulary(object):\n","    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n","    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n","        \"\"\"\n","        매개변수:\n","            token_to_idx(dict): 기존 토큰-인덱스 매핑 딕셔너리\n","            add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n","            unk_token (str): Vocabulary에 추가할 UNK 토큰\n","        \"\"\"\n","\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","            self._token_to_idx = token_to_idx\n","\n","            self._idx_to_token = {idx: token\n","                                  for token, idx in self._token_to_idx.items()}\n","\n","            self._add_unk = add_unk\n","            self._unk_token = unk_token\n","\n","            self.unk_index = -1\n","            if add_unk:\n","                self.unk_index = self.add_token(unk_token)\n","\n","    def to_serializable(self):\n","        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n","        return {'token_to_idx': self._token_to_idx,\n","                'add_unk': self._add_unk,\n","                'unk_token': self._unk_token}\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n","        return cls(**contents)\n","\n","    def add_token(self, token):\n","        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n","\n","        매개변수:\n","            token (str): Vocabulary에 추가할 토큰\n","        반환값:\n","            index (int): 토큰에 상응하는 정수\n","        \"\"\"\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","\n","        def lookup_token(self, token):\n","            \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n","            토큰이 없으면 UNK 인덱스를 반환합니다.\n","\n","            매개변수:\n","                token (str): 찾을 토큰\n","            반환값:\n","                index (int): 토큰에 해당하는 인덱스\n","            노트:\n","                UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n","        'unk_index'가 0보다 커야 합니다.\n","            \"\"\"\n","            if self.add_unk:\n","                return self._token_to_idx.get(token, self.unk_index)\n","            else:\n","                return self._token_to_idx[token]\n","\n","        def lookup_index(self, index):\n","            \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n","\n","            매개변수:\n","                index (int): 찾을 인덱스\n","            반환값:\n","                token (str): 인덱스에 해당하는 토큰\n","            에러:\n","                KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n","            \"\"\"\n","            if index not in self._idx_to_token:\n","                raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n","            return self._idx_to_token[index]\n","\n","        def __str__(self):\n","            return \"<Vocabulary(size=%d)>\" % len(self)\n","\n","        def __len__(self):\n","            return len(self._token_to_idx)\n","\n",""],"metadata":{"id":"3WI1uOfqpK_1"},"execution_count":null,"outputs":[]}]}